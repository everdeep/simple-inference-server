# Server Configuration
PORT=1133
HOST=0.0.0.0
LOG_LEVEL=info

# API Keys (comma-separated for multiple keys)
# Generate keys using: python scripts/generate_api_key.py
API_KEYS=sk-your-secret-key-here,sk-another-key-here
ADMIN_API_KEY=sk-admin-key-with-special-permissions

# Model Configuration
MODEL_PATH=/app/models/your-model.gguf
MODEL_NAME=llama-3-8b

# GPU/CUDA Configuration
N_GPU_LAYERS=-1                    # -1 = offload all layers to GPU, 0 = CPU only
N_CTX=4096                         # Context window size (tokens)
N_BATCH=512                        # Batch size for prompt processing
N_THREADS=8                        # CPU threads (for non-GPU layers)

# Performance Tuning
USE_MLOCK=true                     # Lock model in RAM (prevents swapping)
USE_MMAP=true                      # Memory-mapped file I/O
ROPE_FREQ_BASE=10000.0             # RoPE frequency base (model-specific)
ROPE_FREQ_SCALE=1.0                # RoPE frequency scale (model-specific)

# Docker/CUDA (when using nvidia-docker)
CUDA_VISIBLE_DEVICES=0             # GPU device ID (0 for first GPU)
