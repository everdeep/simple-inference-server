"""
Remote LLM Inference Server
FastAPI-based LLM inference server using llama.cpp with CUDA support.
"""

__version__ = "0.1.0"
